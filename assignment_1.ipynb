{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General libraries useful for python ###\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your working directory is :/net/storage001.ib.cluster/om2/user/smadan/Harvard_BAI/assignment_1\n"
     ]
    }
   ],
   "source": [
    "### Finding where you clone your repo, so that code upstream can work automatically ####\n",
    "machine_path = os.getcwd()\n",
    "work_dir = os.getcwd()\n",
    "print('Your working directory is :%s'%work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Libraries for visualizing our results and data ###\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Import PyTorch ###\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models are being loaded from: /net/storage001.ib.cluster/om2/user/smadan/Harvard_BAI/assignment_1\n",
      "Loaders are being loaded from: /net/storage001.ib.cluster/om2/user/smadan/Harvard_BAI/assignment_1\n"
     ]
    }
   ],
   "source": [
    "### Making helper code under the folder res available. This includes loaders, models, etc. ###\n",
    "sys.path.append('%s/res/'%work_dir)\n",
    "from models.models import get_model\n",
    "from loader.loader import get_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mspandanmadan\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### This code base uses Weights and Biases (wandb.ai) for result visualization. ###\n",
    "### Please make an account at wandb.ai, and follow the steps to login to your account here: ###\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Specifying settings/hyperparameters for our code below ###\n",
    "wandb_config = {}\n",
    "wandb_config['batch_size'] = 10\n",
    "wandb_config['base_lr'] = 0.01\n",
    "wandb_config['model_arch'] = 'CustomCNN'\n",
    "wandb_config['num_classes'] = 10\n",
    "wandb_config['run_name'] = 'TEST_1'\n",
    "wandb_config['use_gpu'] = 1\n",
    "wandb_config['num_epochs'] = 2\n",
    "wandb_config['work_dir'] = work_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load MNIST which can be done easily with PyTorch. The first time you run it, dataset gets downloaded.###\n",
    "if not os.path.isdir('%s/datasets'%work_dir):\n",
    "    os.mkdir('%s/datasets'%work_dir)\n",
    "\n",
    "    \n",
    "data_transforms = {}\n",
    "data_transforms['train'] = torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))])\n",
    "\n",
    "data_transforms['test'] = torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))])\n",
    "    \n",
    "mnist_dataset = {}\n",
    "mnist_dataset['train'] = torchvision.datasets.MNIST('%s/datasets'%work_dir, train = True, download = True, transform = data_transforms['train'])\n",
    "mnist_dataset['test'] = torchvision.datasets.MNIST('%s/datasets'%work_dir, train = False, download = True, transform = data_transforms['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('datasets/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_loaders = {}\n",
    "data_loaders['train'] = torch.utils.data.DataLoader(mnist_dataset['train'], batch_size = wandb_config['batch_size'], shuffle = True)\n",
    "data_loaders['test'] = torch.utils.data.DataLoader(mnist_dataset['test'], batch_size = wandb_config['batch_size'], shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sizes = {}\n",
    "data_sizes['train'] = len(mnist_dataset['train'])\n",
    "data_sizes['test'] = len(mnist_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(wandb_config['model_arch'], wandb_config['num_classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in data_loaders['train']:\n",
    "#     inputs, targets = data\n",
    "#     break\n",
    "\n",
    "# inputs = inputs.cuda()\n",
    "\n",
    "# inputs.shape\n",
    "\n",
    "# outputs = model_arch(inputs)\n",
    "\n",
    "# outputs.shape\n",
    "\n",
    "# targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dset_loaders, dset_sizes, configs):\n",
    "    print('Starting training epoch...')\n",
    "    best_model = model\n",
    "    best_acc = 0.0\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    iters = 0\n",
    "    \n",
    "    for data in tqdm(dset_loaders['train']):\n",
    "        inputs, labels = data\n",
    "        if configs.use_gpu:\n",
    "            inputs = inputs.float().cuda()\n",
    "            labels = labels.long().cuda()\n",
    "        else:\n",
    "            print('WARNING: NOT USING GPU!')\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.long()\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        iters += 1\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        wandb.log({\"train_running_loss\": running_loss/float(iters*len(labels.data))})\n",
    "        wandb.log({\"train_running_corrects\": running_corrects/float(iters*len(labels.data))})\n",
    "\n",
    "    epoch_loss = float(running_loss) / dset_sizes['train']\n",
    "    epoch_acc = float(running_corrects) / float(dset_sizes['train'])\n",
    "    wandb.log({\"train_accuracy\": epoch_acc})\n",
    "    wandb.log({\"train_loss\": epoch_loss})\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, best_acc, best_model, dset_loaders, dset_sizes, configs):\n",
    "    print('Starting testing epoch...')\n",
    "    model.eval()\n",
    "\n",
    "    running_corrects = 0\n",
    "    iters = 0   \n",
    "    for data in tqdm(dset_loaders['test']):\n",
    "        inputs, labels = data\n",
    "        if configs.use_gpu:\n",
    "            inputs = inputs.float().cuda()\n",
    "            labels = labels.long().cuda()\n",
    "        else:\n",
    "            print('WARNING: NOT USING GPU!')\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.long()\n",
    "\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        \n",
    "        iters += 1\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        wandb.log({\"train_running_corrects\": running_corrects/float(iters*len(labels.data))})\n",
    "\n",
    "\n",
    "    epoch_acc = float(running_corrects) / float(dset_sizes['test'])\n",
    "\n",
    "    wandb.log({\"test_accuracy\": epoch_acc})\n",
    "    \n",
    "    if epoch_acc > best_acc:\n",
    "        best_acc = epoch_acc\n",
    "        best_model = copy.deepcopy(model)\n",
    "    wandb.log({\"best_accuracy\": best_acc})\n",
    "    \n",
    "    return best_acc, best_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(model, criterion, optimizer, dset_loaders, dset_sizes, hyperparameters):\n",
    "    with wandb.init(project=\"HARVAR_BAI\", config=hyperparameters):\n",
    "        if hyperparameters['run_name']:\n",
    "            wandb.run.name = hyperparameters['run_name']\n",
    "        config = wandb.config\n",
    "        best_model = model\n",
    "        best_acc = 0.0\n",
    "        \n",
    "        print(config)\n",
    "        \n",
    "        print(config.num_epochs)\n",
    "        for epoch_num in range(config.num_epochs):\n",
    "            wandb.log({\"Current Epoch\": epoch_num})\n",
    "            model = train_model(model, criterion, optimizer, dset_loaders, dset_sizes, config)\n",
    "            best_acc, best_model = test_model(model, best_acc, best_model, dset_loaders, dset_sizes, config)\n",
    "    \n",
    "    return best_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr = wandb_config['base_lr'])\n",
    "\n",
    "if wandb_config['use_gpu']:\n",
    "    criterion.cuda()\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"%s/saved_models/\"%work_dir):\n",
    "    os.mkdir(\"%s/saved_models/\"%work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.15<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">rich-cherry-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/spandanmadan/HARVAR_BAI\" target=\"_blank\">https://wandb.ai/spandanmadan/HARVAR_BAI</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/spandanmadan/HARVAR_BAI/runs/2h59fczz\" target=\"_blank\">https://wandb.ai/spandanmadan/HARVAR_BAI/runs/2h59fczz</a><br/>\n",
       "                Run data is saved locally in <code>/net/storage001.ib.cluster/om2/user/smadan/Harvard_BAI/wandb/run-20210128_155112-2h59fczz</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/6000 [00:00<01:04, 93.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'use_gpu': 1, 'base_lr': 0.01, 'num_epochs': 2, 'num_classes': 10, 'run_name': 'TEST_1', 'model_arch': 'CustomCNN', 'batch_size': 10}\n",
      "2\n",
      "Starting training epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:44<00:00, 134.19it/s]\n",
      "  3%|▎         | 32/1000 [00:00<00:03, 312.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 309.20it/s]\n",
      "  0%|          | 13/6000 [00:00<00:49, 120.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:44<00:00, 133.62it/s]\n",
      "  3%|▎         | 32/1000 [00:00<00:03, 317.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 305.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 47705<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/net/storage001.ib.cluster/om2/user/smadan/Harvard_BAI/wandb/run-20210128_155112-2h59fczz/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/net/storage001.ib.cluster/om2/user/smadan/Harvard_BAI/wandb/run-20210128_155112-2h59fczz/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>_step</td><td>26009</td></tr><tr><td>Current Epoch</td><td>1</td></tr><tr><td>_timestamp</td><td>1611867175</td></tr><tr><td>_runtime</td><td>102</td></tr><tr><td>train_loss</td><td>0.18569</td></tr><tr><td>test_accuracy</td><td>0.5742</td></tr><tr><td>train_accuracy</td><td>0.56663</td></tr><tr><td>best_accuracy</td><td>0.6309</td></tr><tr><td>train_running_corrects</td><td>0.5742</td></tr><tr><td>train_running_loss</td><td>0.18569</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Current Epoch</td><td>▁█</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>test_accuracy</td><td>█▁</td></tr><tr><td>train_loss</td><td>▁█</td></tr><tr><td>train_accuracy</td><td>█▁</td></tr><tr><td>best_accuracy</td><td>▁▁</td></tr><tr><td>train_running_corrects</td><td>▆▅▄▄▄▄▄▅▆▅▅▆▆▆▆▅▅█▇█▅▅▅▃▂▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄</td></tr><tr><td>train_running_loss</td><td>▁▂▃▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▂▂▃▅▇▇███▇▇▇▇▇▆▆▆▆▆▅</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">rich-cherry-1</strong>: <a href=\"https://wandb.ai/spandanmadan/HARVAR_BAI/runs/2h59fczz\" target=\"_blank\">https://wandb.ai/spandanmadan/HARVAR_BAI/runs/2h59fczz</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###########################################################\n",
    "best_final_model = model_pipeline(model, criterion, optimizer_ft, data_loaders, data_sizes, wandb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '%s/saved_models/%s_final.pt'%(work_dir, wandb_config['run_name'])\n",
    "\n",
    "with open(save_path,'wb') as F:\n",
    "    torch.save(best_final_model,F)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bai",
   "language": "python",
   "name": "bai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
